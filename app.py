import streamlit as st
import pandas as pd
import folium
from streamlit_folium import st_folium
from geopy.geocoders import Nominatim
# (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) RateLimiterë¥¼ ì§ì ‘ ì„í¬íŠ¸í•˜ì§€ ì•Šê³  geocode í•¨ìˆ˜ë¥¼ ê°ì‹¸ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
from geopy.extra.rate_limiter import RateLimiter as GeopyRateLimiter  # ì˜ˆì „ ë°©ì‹ì„ ì‹œë„í•´ë³´ê³ 

try:
    from geopy.adapters import RateLimiter  # ìµœì‹  ë°©ì‹ì„ ì‹œë„
except ImportError:
    RateLimiter = GeopyRateLimiter  # ì˜ˆì „ ë°©ì‹ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)

import openai
from openai import OpenAI
import fitz  # PyMuPDF
import os
import re
from pathlib import Path
import requests
import json
import time

# --- 1. ì´ˆê¸° ì„¤ì • (Serper í‚¤ ì¶”ê°€) ---

st.set_page_config(layout="wide")
st.title("ğŸ—ºï¸ ë¼í‹´ì•„ë©”ë¦¬ì¹´ ë‰´ìŠ¤ ê¸°ì‚¬ ì§€ë„ (PDF ê¸°ë°˜)")

# API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    openai_api_key = st.secrets["OPENAI_API_KEY"]
except:
    # ğŸ“Œ ì—¬ê¸°ì— ì‹¤ì œ OpenAI í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë°°í¬ ì‹œì—ëŠ” secrets ì‚¬ìš©)
    openai_api_key = "YOUR OPENAI_API_KEY" # ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”

try:
    serper_api_key = st.secrets["SERPER_API_KEY"]
except:
    # ğŸ“Œ ì—¬ê¸°ì— ì‹¤ì œ Serper í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë°°í¬ ì‹œì—ëŠ” secrets ì‚¬ìš©)
    serper_api_key = "YOUR SERPER API KEY" # ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
if openai_api_key == "YOUR_OPENAI_API_KEY" or not openai_api_key:
    st.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ë” ì•Œì•„ë³´ê¸°' ë° 'ì¢Œí‘œ ê²€ìƒ‰' ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    client = None
else:
    try:
        client = OpenAI(api_key=openai_api_key)
    except Exception as e:
        st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        client = None

# Serper í‚¤ í™•ì¸
if serper_api_key == "YOUR_SERPER_API_KEY" or not serper_api_key:
    st.warning("Serper (Google ê²€ìƒ‰) API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ë” ì•Œì•„ë³´ê¸°' ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 2. ìˆ˜ë™ ìœ„ì¹˜ ìºì‹œ (ì´ì „ê³¼ ë™ì¼) ---
MANUAL_LOCATION_CACHE = {
    "í˜ë£¨, ë¦¬ë§ˆ, Plaza San MartÃ­n": (-12.0505, -77.0339),
    "í˜ë£¨, ë¦¬ë§ˆ": (-12.0464, -77.0428),
    "í˜ë£¨, ë¦¬ë§ˆ, Comas": (-11.9333, -77.0500),
    "í˜ë£¨, ë¦¬ë§ˆ & Callao": (-12.0464, -77.0428),
    "ë³¼ë¦¬ë¹„ì•„, ë¼íŒŒìŠ¤": (-16.4897, -68.1193),
    "ë¯¸êµ­, ì½œë¡œë¼ë„, Aurora": (39.7294, -104.8319),
    "ì•„ë¥´í—¨í‹°ë‚˜": (-38.4161, -63.6167),
    "ë²¨ë¦¬ì¦ˆ": (17.1899, -88.4976),
    "ë³¼ë¦¬ë¹„ì•„": (-16.2902, -63.5887),
    "ë¸Œë¼ì§ˆ": (-14.2350, -51.9253),
    "ì¹ ë ˆ": (-35.6751, -71.5430),
    "ì½œë¡¬ë¹„ì•„": (4.5709, -74.2973),
    "ì½”ìŠ¤íƒ€ë¦¬ì¹´": (9.7489, -83.7534),
    "ì¿ ë°”": (21.5218, -77.7812),
    "ë„ë¯¸ë‹ˆì¹´ ê³µí™”êµ­": (18.7357, -70.1627),
    "ì—ì½°ë„ë¥´": (-1.8312, -78.1834),
    "ì—˜ì‚´ë°”ë„ë¥´": (13.7942, -88.8965),
    "ê³¼í…Œë§ë¼": (15.7835, -90.2308),
    "ì˜¨ë‘ë¼ìŠ¤": (15.2000, -86.2419),
    "ë©•ì‹œì½”": (23.6345, -102.5528),
    "ë‹ˆì¹´ë¼ê³¼": (12.8654, -85.2072),
    "íŒŒë‚˜ë§ˆ": (8.5380, -80.7821),
    "íŒŒë¼ê³¼ì´": (-23.4425, -58.4438),
    "í˜ë£¨": (-9.1900, -75.0152),
    "ìš°ë£¨ê³¼ì´": (-32.5228, -55.7658),
    "ë² ë„¤ìˆ˜ì—˜ë¼": (6.4238, -66.5897),
    "ì•„ì´í‹°": (18.9712, -72.2852),
    "ìë©”ì´ì¹´": (18.1096, -77.2975),
    "í‘¸ì—ë¥´í† ë¦¬ì½”": (18.2208, -66.5901),
    "íŠ¸ë¦¬ë‹ˆë‹¤ë“œ í† ë°”ê³ ": (10.6918, -61.2225),
    "ê°€ì´ì•„ë‚˜": (4.8604, -58.9302),
    "ìˆ˜ë¦¬ë‚¨": (3.9193, -56.0278),
    "í”„ë‘ìŠ¤ë ¹ ê¸°ì•„ë‚˜": (3.9339, -53.1258),
}


# --- 3. (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) PDF íŒŒì‹± ë° ë°ì´í„° ë¡œë”© í•¨ìˆ˜ ---
def parse_pdf_text(text):
    data = {}

    def extract_field(field_name_variations, text_block):
        """(ìˆ˜ì •) í•„ë“œ ì´ë¦„(í‚¤)ì— ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬"""
        for field_name in field_name_variations:
            # 1. ì‰¼í‘œ/ë”°ì˜´í‘œ ê¸°ë°˜ íŒ¨í„´
            pattern_csv = rf'"[^"]*"\s*,\s*"{re.escape(field_name)}"\s*,\s*"([^"]*)"'
            match_csv = re.search(pattern_csv, text_block)
            if match_csv: return match_csv.group(1).strip().strip('""')
            # 2. ì¤„ë°”ê¿ˆ ê¸°ë°˜ íŒ¨í„´ (í‚¤ ì´ë¦„ ìì²´ì˜ ì¤„ë°”ê¿ˆ ì²˜ë¦¬)
            newline_safe_field_name = re.escape(field_name).replace(r'\\\n', r'\s*\\n\s*')
            pattern_newline = rf'\n\d{{1,2}}\n{newline_safe_field_name}\n(.*?)(?=\n\d{{1,2}}\n|\n--- PAGE|\Z)'
            match_newline = re.search(pattern_newline, text_block, re.DOTALL)
            if match_newline:
                value = re.sub(r'\s*\n\s*', ' ', match_newline.group(1)).strip()
                return value
        return "ì •ë³´ ì—†ìŒ"

    data['ëŒ€ë¶„ë¥˜'] = extract_field(["ê°ˆë“± ëŒ€ë¶„ë¥˜"], text)
    data['ì¤‘ë¶„ë¥˜'] = extract_field(["ê°ˆë“± ì¤‘ë¶„ë¥˜"], text)
    data['ì†Œë¶„ë¥˜'] = extract_field(["ê°ˆë“± ì†Œë¶„ë¥˜"], text)

    # (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) ìœ„ì¹˜ ì •ë³´ ì²˜ë¦¬: ìŠ¬ë˜ì‹œ(/)ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    location_str = extract_field(["ìœ„ì¹˜"], text)
    if location_str != "ì •ë³´ ì—†ìŒ" and "/" in location_str:
        data['ì§€ì—­ì •ë³´'] = [loc.strip() for loc in location_str.split('/') if loc.strip()]
    elif location_str != "ì •ë³´ ì—†ìŒ":
        data['ì§€ì—­ì •ë³´'] = [location_str.strip()] # ë‹¨ì¼ ìœ„ì¹˜ë„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    else:
        data['ì§€ì—­ì •ë³´'] = [] # ì •ë³´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸

    data['ê¸°ì‚¬ì œëª©'] = extract_field(["ì œëª©"], text)
    data['ì´ë²¤íŠ¸'] = extract_field(["ë³´ë„ ì¼ì"], text)
    data['original_title'] = extract_field(["ì›ë¬¸ ê¸°ì‚¬ ì œëª©", "ì›ë¬¸ ê¸°ì‚¬ ì œ ëª©", "ì›ë¬¸ ê¸°ì‚¬ ì œ\nëª©"], text)

    url = "ë§í¬ ì—†ìŒ"; found_url = False
    for field_name in ["ì¶œì²˜(URL)"]:
        url_key_match_csv = re.search(rf'"{re.escape(field_name)}"', text)
        if url_key_match_csv:
            text_after_key = text[url_key_match_csv.end():]; url_match = re.search(r'(https?://[^\s)]+)', text_after_key)
            if url_match: url = url_match.group(1).strip().strip(')"'); found_url = True; break
    if not found_url:
        for field_name in ["ì¶œì²˜(URL)"]:
            newline_safe_field_name = re.escape(field_name).replace(r'\\\n', r'\s*\\n\s*');
            pattern_newline_url = rf'\n12\n{newline_safe_field_name}\n[^\n]*\n\((https?://[^\)]+)\)';
            match_newline_url = re.search(pattern_newline_url, text, re.DOTALL)
            if match_newline_url: url = match_newline_url.group(1).strip(); found_url = True; break
    data['ê¸°ì‚¬ë§í¬'] = url

    summary = "ìš”ì•½ ì •ë³´ ì—†ìŒ"; summary_key_variations = [r'ê¸°ì‚¬ í…ìŠ¤íŠ¸\s*\(\s*600ì\s*ì´ë‚´\s*ì¶•ì•½\s*\)', r'ê¸°ì‚¬\s*í…ìŠ¤íŠ¸\s*\(600ì\s*ì´ë‚´\s*\n\s*ì¶•ì•½\)', r'"ê´€ë ¨ ì´ë²¤íŠ¸"']; summary_key_pattern = f'({"|".join(summary_key_variations)})'; summary_key_match = re.search(summary_key_pattern, text, re.DOTALL | re.IGNORECASE)
    if summary_key_match:
        text_after_key = text[summary_key_match.end():]
        summary_content_match = re.search(r'^\s*(.*?[\uAC00-\uD7A3]\s*ë‹¤\.)\s*(?=\n[A-ZÃ€-Ã¿a-z]|\n--- PAGE|\Z)', text_after_key, re.DOTALL | re.MULTILINE)
        if summary_content_match:
            summary_raw = summary_content_match.group(1).strip(); summary = re.sub(r'^,,', '', summary_raw.strip(), flags=re.MULTILINE); summary = re.sub(r'\s*\n\s*', ' ', summary); summary = summary.strip().strip('"')
        elif '"ê´€ë ¨ ì´ë²¤íŠ¸"' in summary_key_match.group(1):
            summary_match_arg = re.search(r'^\s*,\s*,(.*?)(?=\n"\d{1,2}"\s*,|\n,,"ê¸°ì‚¬ í…ìŠ¤íŠ¸")', text_after_key, re.DOTALL)
            if summary_match_arg:
                summary_raw = summary_match_arg.group(1); summary_test = re.sub(r'^\s*,,', '', summary_raw, flags=re.MULTILINE).strip().strip('"'); summary_test = re.sub(r'\s*\n\s*', ' ', summary_test)
                if summary_test.endswith('ë‹¤.'): summary = summary_test
    data['ìš”ì•½'] = summary; data['ë²ˆì—­'] = summary
    for key, value in data.items():
        # ì§€ì—­ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ is False ëŒ€ì‹  not value ì‚¬ìš©
        if key != 'ì§€ì—­ì •ë³´' and not value: data[key] = "ì •ë³´ ì—†ìŒ"
        elif key == 'ì§€ì—­ì •ë³´' and not value: data[key] = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€
    return data


@st.cache_data
def load_data_from_pdfs(folder_path="sampledata"):
    all_articles = []; data_folder = Path(folder_path); first_pdf_text = None
    if not data_folder.exists() or not data_folder.is_dir(): st.error(f"'{folder_path}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame(), None
    pdf_files = list(data_folder.glob("*.pdf"))
    if not pdf_files: st.error(f"'{folder_path}' í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame(), None
    progress_bar = st.progress(0, text="PDF íŒŒì¼ ë¡œë”© ì¤‘...")
    for i, pdf_path in enumerate(pdf_files):
        try:
            doc = fitz.open(pdf_path); full_text = "".join(page.get_text("text", sort=False) for page in doc); doc.close()
            if i == 0: first_pdf_text = full_text
            article_data = parse_pdf_text(full_text); article_data['filename'] = pdf_path.name; all_articles.append(article_data)
            progress_bar.progress((i + 1) / len(pdf_files), text=f"PDF íŒŒì¼ ë¡œë”© ì¤‘: {pdf_path.name}")
        except Exception as e: st.warning(f"'{pdf_path.name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    progress_bar.empty()
    if not all_articles: return pd.DataFrame(), first_pdf_text
    df = pd.DataFrame(all_articles);
    # ì§€ì—­ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ê²€ì‚¬ ë°©ì‹ ë³€ê²½
    required_cols = ['ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì§€ì—­ì •ë³´', 'ê¸°ì‚¬ì œëª©', 'original_title', 'ì´ë²¤íŠ¸', 'ë²ˆì—­', 'ìš”ì•½']; all_cols_valid = True
    for col in required_cols:
        if col not in df.columns: df[col] = [] if col == 'ì§€ì—­ì •ë³´' else "ì •ë³´ ì—†ìŒ"; all_cols_valid = False
        elif col != 'ì§€ì—­ì •ë³´' and ((df[col] == "ì •ë³´ ì—†ìŒ").all() or df[col].isnull().all()): all_cols_valid = False
        # ì§€ì—­ì •ë³´ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ë§Œ ëª¨ë“  í–‰ì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼
        elif col == 'ì§€ì—­ì •ë³´' and all(not x for x in df[col]): all_cols_valid = False

    if not all_cols_valid: return df, first_pdf_text
    return df, None


# --- 4. ì§€ì˜¤ì½”ë”© ë¡œì§ (ì´ì „ê³¼ ë™ì¼) ---
geolocator = Nominatim(user_agent="Mozilla/5.0")
# RateLimiterê°€ importë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìˆ˜ë™ ì§€ì—° ì‚¬ìš©
geocode_nominatim = RateLimiter(geolocator.geocode, min_delay_seconds=1.1) if RateLimiter else geolocator.geocode

def get_coords_via_openai(location_str):
    if not client: print("OpenAI client not initialized."); return None, None
    prompt = f"ë‹¤ìŒ ì¥ì†Œì˜ ìœ„ë„(latitude)ì™€ ê²½ë„(longitude)ë¥¼ 'latitude, longitude' í˜•ì‹ìœ¼ë¡œ ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ì•Œë ¤ì£¼ì„¸ìš”. ëª¨ë¥´ë©´ 'None, None'ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.\nì¥ì†Œ: \"{location_str}\"\nì¢Œí‘œ:"
    try:
        response = client.chat.completions.create(model="gpt-4o", messages=[{"role": "system", "content": "You provide geographical coordinates."}, {"role": "user", "content": prompt}], max_tokens=20, temperature=0.0)
        time.sleep(1.1) # OpenAI í˜¸ì¶œ í›„ì—ë„ ì•½ê°„ì˜ ì§€ì—° ì¶”ê°€ (API í˜¸ì¶œ ì œí•œ ë°©ì§€)
        result_text = response.choices[0].message.content.strip()
        coords = result_text.split(',')
        if len(coords) == 2:
            lat_str, lon_str = coords[0].strip(), coords[1].strip()
            try:
                if lat_str.lower() != 'none' and lon_str.lower() != 'none':
                    lat, lon = float(lat_str), float(lon_str); print(f"OpenAI Geocoding SUCCESS for '{location_str}': ({lat}, {lon})"); return lat, lon
                else: print(f"OpenAI Geocoding: Could not find coordinates for '{location_str}'"); return None, None
            except ValueError: print(f"OpenAI Geocoding: Could not parse coordinates from '{result_text}' for '{location_str}'"); return None, None
        else: print(f"OpenAI Geocoding: Unexpected response format for '{location_str}': {result_text}"); return None, None
    except Exception as e: print(f"OpenAI Geocoding Error for '{location_str}': {e}"); return None, None

@st.cache_data
def get_lat_lon(location_str):
    if location_str == "ì •ë³´ ì—†ìŒ" or not location_str: return None, None
    if location_str in MANUAL_LOCATION_CACHE: return MANUAL_LOCATION_CACHE[location_str]

    clean_str = re.sub(r'\s*&.*', '', location_str).strip()
    lat_openai, lon_openai = None, None # OpenAI ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜

    try:
        # RateLimiter ì ìš© (import ì„±ê³µ ì‹œ) ë˜ëŠ” ì§ì ‘ í˜¸ì¶œ (ì‹¤íŒ¨ ì‹œ)
        location = geocode_nominatim(clean_str, timeout=10)
        if not RateLimiter: # RateLimiter ì—†ìœ¼ë©´ ìˆ˜ë™ ì§€ì—°
            time.sleep(1.1)
        if location:
            print(f"Geopy SUCCESS for '{location_str}': ({location.latitude}, {location.longitude})"); return location.latitude, location.longitude
        else:
            print(f"Geopy: Location not found for '{location_str}'")
            lat_openai, lon_openai = get_coords_via_openai(location_str)
            if lat_openai is not None: return lat_openai, lon_openai

    except Exception as e:
        print(f"Geopy Error for '{location_str}': {e}")
        lat_openai, lon_openai = get_coords_via_openai(location_str)
        if lat_openai is not None: return lat_openai, lon_openai

    if lat_openai is None:
        try:
            country_name = location_str.split(',')[0].strip()
            if country_name in MANUAL_LOCATION_CACHE: return MANUAL_LOCATION_CACHE[country_name]
            location_country = geocode_nominatim(country_name, timeout=10)
            if not RateLimiter: # RateLimiter ì—†ìœ¼ë©´ ìˆ˜ë™ ì§€ì—°
                time.sleep(1.1)
            if location_country:
                print(f"Geopy Country Fallback SUCCESS for '{location_str}' -> '{country_name}': ({location_country.latitude}, {location_country.longitude})"); return location_country.latitude, location_country.longitude
        except Exception as e: print(f"Geopy Country Fallback Error for '{country_name}': {e}")

    print(f"All Geocoding attempts FAILED for '{location_str}'")
    return None, None


# --- 5. Serper Google ê²€ìƒ‰ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def call_google_search(query, api_key):
    url = "https://google.serper.dev/search"; payload = json.dumps({"q": query, "gl": "us", "hl": "ko"}); headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=payload, timeout=10); response.raise_for_status(); results = response.json(); organic_results = results.get('organic', []); formatted_results = []
        for item in organic_results[:5]: formatted_results.append({"title": item.get('title'), "link": item.get('link'), "snippet": item.get('snippet')})
        return formatted_results
    except requests.exceptions.RequestException as e: st.error(f"Google ê²€ìƒ‰ API(Serper) í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return None


# --- 6. (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---

df, debug_text = load_data_from_pdfs("sampledata")

if debug_text:
    st.error("ë°ì´í„° íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì‹± ë¡œì§ì´ PDF êµ¬ì¡°ì™€ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.subheader("ë””ë²„ê¹… ì •ë³´: ì²« ë²ˆì§¸ PDF ì¶”ì¶œ ì›ë³¸ í…ìŠ¤íŠ¸ (ì¼ë¶€)")
    st.text_area("Raw Text", debug_text[:2000], height=300)
    # ì§€ì—­ì •ë³´ í¬í•¨í•˜ì—¬ ì‹¤íŒ¨í•œ ì»¬ëŸ¼ í‘œì‹œ
    for col in ['ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì§€ì—­ì •ë³´', 'ê¸°ì‚¬ì œëª©', 'original_title', 'ì´ë²¤íŠ¸', 'ìš”ì•½']:
        if col not in df: st.warning(f"ê²½ê³ : '{col}' ì»¬ëŸ¼ ìì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        elif col != 'ì§€ì—­ì •ë³´' and ((df[col] == "ì •ë³´ ì—†ìŒ").all() or df[col].isnull().all()): st.warning(f"ê²½ê³ : '{col}' ì»¬ëŸ¼ì˜ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        elif col == 'ì§€ì—­ì •ë³´' and all(not x for x in df[col]): st.warning(f"ê²½ê³ : '{col}' ì»¬ëŸ¼ì˜ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


elif df.empty or all(not x for x in df['ì§€ì—­ì •ë³´']): # ì§€ì—­ì •ë³´ê°€ ëª¨ë“  í–‰ì—ì„œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    st.error("ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ìœ íš¨í•œ 'ì§€ì—­ì •ë³´'ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.success(f"ì´ {len(df)}ê°œì˜ PDF ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•˜ê³  íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")
    keyword = st.text_input("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í˜ë£¨, ì¸í”Œë ˆì´ì…˜, ë¦¬ë§ˆ ë“±)", "")

    if keyword:
        # (ìˆ˜ì •) ê²€ìƒ‰ ê°€ëŠ¥ í•„í„°: ì§€ì—­ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ë§Œ í™•ì¸
        df_searchable = df[
            (df['ëŒ€ë¶„ë¥˜'] != "ì •ë³´ ì—†ìŒ") &
            (df['ê¸°ì‚¬ì œëª©'] != "ì •ë³´ ì—†ìŒ") &
            (df['original_title'] != "ì •ë³´ ì—†ìŒ") &
            (df['ì§€ì—­ì •ë³´'].apply(lambda x: bool(x))) # ì§€ì—­ì •ë³´ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ë§Œ
        ]
        # (ìˆ˜ì •) ê²€ìƒ‰ ì¡°ê±´(mask): ì§€ì—­ì •ë³´ëŠ” ë¦¬ìŠ¤íŠ¸ ë‚´ ê° í•­ëª©ì— ëŒ€í•´ ê²€ìƒ‰
        mask = (
                df_searchable['ëŒ€ë¶„ë¥˜'].str.contains(keyword, case=False, na=False) |
                df_searchable['ì¤‘ë¶„ë¥˜'].str.contains(keyword, case=False, na=False) |
                df_searchable['ì†Œë¶„ë¥˜'].str.contains(keyword, case=False, na=False) |
                df_searchable['ê¸°ì‚¬ì œëª©'].str.contains(keyword, case=False, na=False) |
                df_searchable['original_title'].str.contains(keyword, case=False, na=False) |
                # ì§€ì—­ì •ë³´ ë¦¬ìŠ¤íŠ¸ ë‚´ ê° í•­ëª©ì— ëŒ€í•´ keyword í¬í•¨ ì—¬ë¶€ í™•ì¸
                df_searchable['ì§€ì—­ì •ë³´'].apply(lambda loc_list: any(keyword.lower() in loc.lower() for loc in loc_list))
        )
        filtered_df = df_searchable[mask].copy()

        if filtered_df.empty:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ê²€ìƒ‰ëœ ì§€ì—­ì˜ ì¢Œí‘œë¥¼ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤...")
            geocoding_placeholder = st.empty(); log_messages = []; location_cache = {}

            # (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) ê³ ìœ  ìœ„ì¹˜ ëª©ë¡ ìƒì„±: ë¦¬ìŠ¤íŠ¸ë¥¼ í¼ì³ì„œ ìƒì„±
            unique_locations = set()
            for loc_list in filtered_df['ì§€ì—­ì •ë³´']:
                unique_locations.update(loc_list) # setì— ì¶”ê°€í•˜ì—¬ ìë™ ì¤‘ë³µ ì œê±°
            unique_locations = list(unique_locations) # ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            total_locations = len(unique_locations)

            progress_bar = st.progress(0, text="ì¢Œí‘œ ë³€í™˜ ì‹œì‘...")
            for i, location_str in enumerate(unique_locations):
                progress_text = f"ë³€í™˜ ì¤‘ ({i+1}/{total_locations}): {location_str}"
                if location_str in MANUAL_LOCATION_CACHE: progress_text += " (ìˆ˜ë™ ìºì‹œ ì‚¬ìš©)"
                else: progress_text += " (Geopy/OpenAI ì‹œë„ ì¤‘...)"
                progress_bar.progress((i + 1) / total_locations, text=progress_text)
                lat, lon = get_lat_lon(location_str); location_cache[location_str] = (lat, lon) # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
                method_used = "ìˆ˜ë™ ìºì‹œ" if location_str in MANUAL_LOCATION_CACHE else "Geopy/OpenAI"
                if lat is not None: log_messages.append(f"âœ… **[ì„±ê³µ]** `{location_str}` -> `({lat:.4f}, {lon:.4f})` (ë°©ë²•: {method_used})")
                else: log_messages.append(f"âŒ **[ì‹¤íŒ¨]** `{location_str}` -> ëª¨ë“  ë°©ë²•(ìˆ˜ë™, Geopy, OpenAI, êµ­ê°€ëª…) ì‹¤íŒ¨")
            geocoding_placeholder.expander("ì¢Œí‘œ ë³€í™˜ ë¡œê·¸ ë³´ê¸°", expanded=True).markdown("\n".join(log_messages))
            progress_bar.empty()

            # (â˜…â˜…â˜… ìˆ˜ì •ë¨ â˜…â˜…â˜…) ì§€ë„ í‘œì‹œ ë¡œì§: ê° ê¸°ì‚¬ì˜ ê° ìœ„ì¹˜ì— ë§ˆì»¤ ìƒì„±
            map_data = [] # ì§€ë„ì— í‘œì‹œí•  ë°ì´í„° (ë§ˆì»¤ ì¤‘ë³µ ë°©ì§€ìš©)
            has_valid_location = False # ìœ íš¨í•œ ì¢Œí‘œê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸

            for idx, row in filtered_df.iterrows():
                for location_str in row['ì§€ì—­ì •ë³´']:
                    coords = location_cache.get(location_str)
                    if coords and coords[0] is not None:
                        has_valid_location = True
                        # ë™ì¼ ê¸°ì‚¬, ë™ì¼ ìœ„ì¹˜ì— ë§ˆì»¤ ì¤‘ë³µ ìƒì„± ë°©ì§€
                        map_key = f"{idx}_{location_str}"
                        if map_key not in [d['key'] for d in map_data]:
                            map_data.append({
                                'key': map_key,
                                'latitude': coords[0],
                                'longitude': coords[1],
                                'popup_data': row # ë§ˆì»¤ ìƒì„±ì— í•„ìš”í•œ ì „ì²´ í–‰ ë°ì´í„°
                            })

            if not has_valid_location:
                st.warning("í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ëŠ” ìˆìœ¼ë‚˜, ì§€ë„ì— í‘œì‹œí•  ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ìœ„ì˜ 'ì¢Œí‘œ ë³€í™˜ ë¡œê·¸'ë¥¼ í™•ì¸í•˜ì—¬ ëª¨ë“  ìœ„ì¹˜ê°€ âŒ[ì‹¤íŒ¨]í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
            else:
                geocoding_placeholder.empty()

                # 7. Folium ì§€ë„ ì‹œê°í™” (ë§ˆì»¤ ìƒì„± ë¡œì§ ìˆ˜ì •)
                # (ìˆ˜ì •) ì§€ë„ ì¤‘ì‹¬ ê³„ì‚°: map_dataì— ìˆëŠ” ëª¨ë“  ìœ íš¨ ì¢Œí‘œ ì‚¬ìš©
                avg_lat = sum(d['latitude'] for d in map_data) / len(map_data)
                avg_lon = sum(d['longitude'] for d in map_data) / len(map_data)
                m = folium.Map(location=[avg_lat, avg_lon], zoom_start=4)

                color_map = {'êµ­ë‚´(ì‚¬íšŒ)': 'red', 'êµ­ë‚´(ê²½ì œ)': 'green', 'êµ­ë‚´(ë²”ì£„)': 'black', 'êµ­ì œ(êµ­ì œê´€ê³„)': 'purple', 'ì •ì¹˜': 'blue'}

                # (ìˆ˜ì •) map_dataë¥¼ ìˆœíšŒí•˜ë©° ë§ˆì»¤ ìƒì„±
                for data_point in map_data:
                    row_data = data_point['popup_data'] # í•´ë‹¹ ë§ˆì»¤ì˜ ì›ë³¸ ê¸°ì‚¬ ë°ì´í„°
                    popup_html = f"""
                    <h4>{row_data['ê¸°ì‚¬ì œëª©']}</h4>
                    <i>{row_data['original_title']}</i><br><br>
                    <b>ì‹œê°„:</b> {row_data['ì´ë²¤íŠ¸']}<br>
                    <b>ë¶„ë¥˜:</b> {row_data['ëŒ€ë¶„ë¥˜']} > {row_data['ì¤‘ë¶„ë¥˜']} > {row_data['ì†Œë¶„ë¥˜']}<br>
                    <a href="{row_data['ê¸°ì‚¬ë§í¬']}" target="_blank">ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°</a>
                    <hr>
                    <div id="details_{data_point['key']}" style="display:none; max-height: 150px; overflow-y: auto;">
                        <b>ìš”ì•½:</b><p>{row_data['ìš”ì•½']}</p>
                    </div>
                    <button onclick="
                        var el = document.getElementById('details_{data_point['key']}');
                        if (el.style.display == 'none') {{
                            el.style.display = 'block'; this.textContent = 'ìš”ì•½ ë‹«ê¸°';
                        }} else {{
                            el.style.display = 'none'; this.textContent = 'ìš”ì•½ ë³´ê¸°';
                        }}
                    ">ìš”ì•½ ë³´ê¸°</button>
                    """
                    iframe = folium.IFrame(popup_html, width=350, height=280)
                    popup = folium.Popup(iframe, max_width=350)
                    folium.Marker(
                        location=[data_point['latitude'], data_point['longitude']],
                        popup=popup,
                        icon=folium.Icon(color=color_map.get(row_data['ëŒ€ë¶„ë¥˜'], 'gray')),
                        tooltip=row_data['ê¸°ì‚¬ì œëª©'] # íˆ´íŒì€ í•œêµ­ì–´ ì œëª© ìœ ì§€
                    ).add_to(m)

                st.subheader(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(filtered_df)}ê°œ ê¸°ì‚¬ / {len(map_data)}ê°œ ìœ„ì¹˜") # í‘œì‹œ ì •ë³´ ìˆ˜ì •
                st_folium(m, width='100%', height=500)

                # 8. OpenAI + Serper ì—°ë™ (ê²€ìƒ‰ì–´ ë¡œì§: í•œêµ­ì–´/ìŠ¤í˜ì¸ì–´ ë¶„ë¦¬)
                st.markdown("---")
                if st.button("ğŸ¤– AIë¡œ ìœ ì‚¬ ê¸°ì‚¬ ë” ì•Œì•„ë³´ê¸° (ì‹¤ì œ ê²€ìƒ‰)"):
                    if not client:
                        st.error("OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                    elif serper_api_key == "YOUR_SERPER_API_KEY" or not serper_api_key:
                        st.error("Serper (Google ê²€ìƒ‰) API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    else:
                        with st.spinner("AIê°€ Googleì—ì„œ ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³  ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤..."):
                            # 1) ì†Œë¶„ë¥˜ í‚¤ì›Œë“œ ìµœëŒ€ 5ê°œ ìˆ˜ì§‘
                            sub_categories = set()
                            try:
                                if 'ì†Œë¶„ë¥˜' in filtered_df.columns:
                                    sub_categories.update(filtered_df['ì†Œë¶„ë¥˜'].dropna().unique().tolist())
                            except Exception:
                                pass
                            sub_category_keywords_ko = [cat.strip() for cat in sub_categories if cat and cat != "ì •ë³´ ì—†ìŒ"]
                            sub_category_keywords_ko = sub_category_keywords_ko[:5]

                            # 2) í•œêµ­ì–´ ê²€ìƒ‰ì–´
                            search_query_ko = f"ë¼í‹´ì•„ë©”ë¦¬ì¹´, ì¤‘ë‚¨ë¯¸, ë‰´ìŠ¤, ê¸°ì‚¬, {keyword}".strip().strip(",")


                            # 3) ìŠ¤í˜ì¸ì–´ ê²€ìƒ‰ìš©: keyword + ì†Œë¶„ë¥˜ë¥¼ ê°„ë‹¨ ë²ˆì—­
                            def translate_to_es(text_list):
                                if not text_list:
                                    return []
                                try:
                                    msg = [
                                        {"role": "system",
                                         "content": "You are a concise translator from Korean to Spanish."},
                                        {"role": "user",
                                         "content": "ë‹¤ìŒ í•­ëª©ë“¤ì„ ìŠ¤í˜ì¸ì–´ë¡œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ ì£¼ì„¸ìš”. ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë°˜í™˜: " + ", ".join(
                                             text_list)}
                                    ]
                                    tr = client.chat.completions.create(
                                        model="gpt-4o",
                                        messages=msg,
                                        temperature=0
                                    )
                                    out = tr.choices[0].message.content or ""
                                    # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ & ê³µë°± íŠ¸ë¦¬ë°
                                    return [t.strip() for t in out.split(",") if t.strip()]
                                except Exception:
                                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì‚¬ìš©
                                    return text_list


                            keyword_es_list = translate_to_es([str(keyword)]) if keyword else []
                            sub_category_keywords_es = translate_to_es(sub_category_keywords_ko)

                            keyword_es = keyword_es_list[0] if keyword_es_list else ""
                            base_es_terms = ["AmÃ©rica Latina", "LatinoamÃ©rica", "noticias", "artÃ­culo"]
                            # 4) ìŠ¤í˜ì¸ì–´ ê²€ìƒ‰ì–´
                            search_query_es = f"{', '.join(base_es_terms)}, {keyword_es} " + " ".join(
                                sub_category_keywords_es)


                            # 5) ê³µí†µ: ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜
                            def run_search_and_summarize(search_query, lang_label="KO"):
                                st.markdown(f"### {'ğŸ‡°ğŸ‡· í•œêµ­ì–´' if lang_label == 'KO' else 'ğŸ‡ªğŸ‡¸ ìŠ¤í˜ì¸ì–´'} ê²€ìƒ‰")
                                st.text(f"(ê²€ìƒ‰ì–´: {search_query})")

                                results = call_google_search(search_query, serper_api_key)
                                if not results:
                                    st.error("Google ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    return

                                search_context = ""
                                for i, res in enumerate(results):
                                    title = res.get('title', '')
                                    link = res.get('link', '')
                                    snippet = res.get('snippet', '')
                                    search_context += f"--- Result {i + 1} ---\nTitle: {title}\nLink: {link}\nSnippet: {snippet}\n"

                                # í”„ë¡¬í”„íŠ¸: ì¶œë ¥ì€ í•œêµ­ì–´ ìš”ì•½ ìœ ì§€ (ì›í•˜ì‹œë©´ ìŠ¤í˜ì¸ì–´ ì„¹ì…˜ë§Œ ìŠ¤í˜ì¸ì–´ ìš”ì•½ìœ¼ë¡œ ë°”ê¿”ë„ ë©ë‹ˆë‹¤)
                                prompt = f"""ë‹¹ì‹ ì€ ë¼í‹´ì•„ë©”ë¦¬ì¹´ ì „ë¬¸ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆìœ¼ë©°, ì•„ë˜ëŠ” Google ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.
                <Google ê²€ìƒ‰ ê²°ê³¼>
                {search_context}
                </Google ê²€ìƒ‰ ê²°ê³¼>
                ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬ ê¸°ì‚¬ 3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. ë‹¤ìŒ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”.
                - **ê¸°ì‚¬ ì œëª©:** [ì‹¤ì œ ì œëª©]
                - **ê¸°ì‚¬ ë§í¬:** [ì‹¤ì œ ë§í¬]
                - **ë²ˆì—­ ë° ìš”ì•½:** [Snippet ë°”íƒ• AI ìƒì„± í•œêµ­ì–´ ìš”ì•½]
                ---"""

                                try:
                                    response = client.chat.completions.create(
                                        model="gpt-4o",
                                        messages=[
                                            {"role": "system",
                                             "content": "You are a helpful assistant specializing in Latin American news."},
                                            {"role": "user", "content": prompt}
                                        ],
                                        temperature=0.2
                                    )
                                    result_text = response.choices[0].message.content
                                    st.subheader("AI ì¶”ì²œ ìœ ì‚¬ ê¸°ì‚¬ (ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼)")
                                    st.markdown(result_text)
                                except Exception as e:
                                    st.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


                            # 6) ë‘ ì–¸ì–´ ê²€ìƒ‰ ê°ê° ì‹¤í–‰
                            run_search_and_summarize(search_query_ko, lang_label="KO")
                            run_search_and_summarize(search_query_es, lang_label="ES")

    # --- ì•± í•˜ë‹¨ ì €ì‘ê¶Œ ì •ë³´ (ì´ì „ê³¼ ë™ì¼) ---
    st.markdown("---")
    st.markdown(
        """<div style="text-align: center; color: grey; font-size: 0.8em;">
        This database and news map were created by the Institute for Spanish and Latin American Studies (HK+ Program) at Korea University.
        </div>""",
        unsafe_allow_html=True
    )

